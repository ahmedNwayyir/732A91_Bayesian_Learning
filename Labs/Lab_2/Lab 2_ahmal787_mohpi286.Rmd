---
title: |
  | Bayesian Learning: Lab 2
author: |
  | Mohsen Pirmoradiyan, Ahmed Alhasan
date: "`r Sys.Date()`"
geometry: "left=2.5cm,right=2.5cm,top=2cm,bottom=2cm"
output:
  pdf_document: 
header-includes: 
  \usepackage{xcolor}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center", warning = FALSE, out.width = "80%", fig.height=3.5)
knitr::read_chunk("R_Code.r")
```

# 1. Linear and polynomial regression

The dataset `TempLinkoping.txt` contains daily average temperatures (in Celcius degrees) at Malmslätt, Linköping over the course of the year 2018. The response variable is *temp* and the covariate is

$$time = \frac{\text{the number of days since beginning of year}}{365}$$
The task is to perform a Bayesian analysis of a quadratic regression

$$ temp = \beta_0 + \beta_1 \cdot time + \beta_2 \cdot time^2 + \epsilon, \epsilon \stackrel{iid}{\sim} \mathcal{N}(0,\sigma^2).$$

(a) *Determining the prior distribution of the model parameters*. Use the conjugate prior for the linear regression model. Your task is to set the prior hyperparameters $\mu_0, \Omega_0,\nu_0$ and $\sigma_0^2$ to sensible values. Start with $\mu_0 = (-10, 100, -100)^T$ , $\Omega_0 = 0.01 \cdot I_3$, $\nu_0=4$ and $\sigma_0^2 = 1$. Check if this prior agrees with your prior opinions by simulating draws from the joint prior of all parameters and for every draw compute the regression curve. This gives a collection of regression curves, one for each draw from the prior. Do the collection of curves look reasonable? If not, change the prior hyperparameters until the collection of prior regression curves agrees with your prior beliefs about the regression curve. [Hint: the R package `mvtnorm` will be handy. And use your $Inv-\chi^2$ simulator from Lab 1.]

\definecolor{mycolor}{HTML}{00007D}
- \textcolor{mycolor}{The Provided set of hyper-parameters gave a wide range of uncertainty of what might the temperature be in Linkoping in a year which does not agree with our belief that the temperature has less variance than this i.e. the temperature in summer cannot be in minus or very high in winter.}
```{r 1_a1}
```

- \textcolor{mycolor}{We have increased the precision hyper-parameter $\Omega_0$ to 0.15 to reduce this variance otherwise we could do that by reducing the variance hyper-paramter itself} 

- \textcolor{mycolor}{Also we changed the mean of the intercept which represent the temperature of day 1 in the year, January, to reflect our belief of what might be the average temperature that date, Changed $\beta_1$ and $\beta_2$ to 98 and -96 respectively to make the middle hump higher "temperatures in summer between 10-30C
and the end of the year similar to the begining of the year}

- \textcolor{mycolor}{And increased $\nu_0$ to 100 to give us more control of the outcome of model variance "more certainty"}
```{r 1_a2}
```

- \textcolor{mycolor}{After we have made our opnion about the prior we could plot the data that we will use as well to compute the posterior in the next step}
```{r 1_a3}
```


(b) Write a program that *simulates from the joint posterior distribution* of $\beta_0, \beta_1, \beta_2$ and $\sigma^2$. Plot the marginal posteriors for each parameter as a histogram. Also produce another figure with a scatter plot of the temperature data and overlay a curve for the posterior median of the regression function $f(time) = \beta_0 + \beta_1 \cdot time + \beta_2 \cdot time^2$, computed for every value of *time*. Also overlay curves for the lower 2.5% and upper 97.5% posterior credible interval for *f(time)*. That is, compute the 95% equal tail posterior probability intervals for every value of *time* and then connect the lower and upper limits of the interval by curves. Does the interval bands contain most of the data points? Should they?


$$\color{mycolor}{\sigma^2 \mid y \sim Inv-\chi^2(\nu_n,\sigma_n^2)}$$
$$\color{mycolor}{\beta \mid \sigma^2,y \sim \mathcal N(\mu_n,\sigma^2 \Omega_n^{-1})}$$

\begin{equation*}
\everymath{\color{mycolor}}
  \begin{split}
    \Omega_n =& \; X^TX + \Omega_0 \\
    \mu_n =& \; (X^TX + \Omega_0)^{-1} (X^TX\hat{\beta} + \Omega_0\mu_0) \\
    \nu_n =& \; \nu_0 + n \\
    \nu_n \sigma_n^2 =& \; \nu_0\sigma_0^2 + (y^Ty + \mu_0^T\Omega_0\mu_0 - \mu_n^T\Omega_n\mu_n)
  \end{split}
\end{equation*}

```{r 1_b1}
```

```{r 1_b2}
```
- \textcolor{mycolor}{The credible interval does not contain most of the data points and it shouldn't be since it is not a prediction interval, it is the interval that the median fall into with proabability=95 percent}

(c) It is of interest to locate the *time* with the highest expected temperature (that is, the *time* where f(*time*) is maximal). Let's call this value $\tilde{x}$. Use the simulations in b) to simulate from the *posterior distribution of $\tilde{x}$*. [Hint: the regression curve is a quadratic. You can find a simple formula for $\tilde{x}$ given $\beta_0, \beta_1$ and $\beta_2$.]

- \textcolor{mycolor}{We can get the maximum temperature by taking the derivative in respect to time and setting it to 0}

\begin{equation*}
\everymath{\color{mycolor}}
  \begin{split}
    temp =& \; \beta_0 + \beta_1 \cdot time + \beta_2 \cdot time^2 \\
    \frac{temp}{\partial time} =& \; \beta_1 + 2 \; \beta_2 \; time \\
    \tilde{x} =& \; -\frac{\beta_1}{2\beta_2}
  \end{split}
\end{equation*}

```{r 1_c}
```


(d) Say now that you want to *estimate a polynomial model of order* 7, but you suspect that higher order terms may not be needed, and you worry about overfitting. Suggest a suitable prior that mitigates this potential problem. You do not need to compute the posterior, just write down your prior. [Hint: the task is to specify $\mu_0$ and $\Omega_0$ in a smart way.]

- \textcolor{mycolor}{We can do that by making the prior mean "8 dimensional vector" far from the data mean and increasing the precision hyper-parameter $\Omega$ 8x8 matrix each by a constant depending how much we want to penalize the data since they make a strong informative prior that pull the posterior out from the data}


\newpage
# 2. Posterior approximation for classification with logistic regression

The dataset `WomenWork.dat` contains n = 200 observations (i.e. women) on the following nine variables:

\begin{center}
 \begin{tabular}{|c | c | c | c|}
 \hline
 Variable & Data type & Meaning & Role \\ [0.5ex] 
 \hline\hline
 Work & Binary & Whether or not the woman works & Response \\ [0.5ex] 
 \hline
 Constant & 1 & Constant to the intercept & Feature \\ [0.5ex] 
 \hline
 HusbandInc & Numeric & Husband’s income & Feature \\ [0.5ex] 
 \hline
 EducYears & Counts & Years of education & Feature \\ [0.5ex] 
 \hline
 ExpYears & Counts & Years of experience & Feature \\ [0.5ex] 
 \hline
 ExpYears2 & Numeric & (Years of experience/$10)^2$ & Feature \\ [0.5ex] 
 \hline
 Age & Counts & Age & Feature \\ [0.5ex] 
 \hline
 NSmallChild & Counts & Number of child $\leq$ 6 years in household & Feature \\ [0.5ex] 
 \hline
 NBigChild & Counts & Number of child > 6 years in household & Feature \\ [0.5ex]  
 \hline
\end{tabular}
\end{center}

(a) Consider the logistic regression

$$Pr(y=1|x) = \frac{exp(x^T\beta)}{1 + exp(x^T\beta)}$$

> where y is the binary variable with $y = 1$ if the woman works and $y = 0$ if she does not. x is a 8-dimensional vector containing the eight features (including a one for the constant term that models the intercept). 

> The goal is to approximate the posterior distribution of the 8-dim parameter vector $\beta$ with a multivariate normal distribution

$$\beta|y,X \sim \mathcal{N}\left(\tilde{\beta}, J^{(-1)}_y(\tilde{\beta})\right),$$

>where $\tilde{\beta}$ is the posterior mode and $J(\tilde{\beta}) = - \frac{\partial^2 \;\text{ln} \; p(\beta|\text{y})}{\partial \beta \partial \beta^T}|_{\beta = \tilde{\beta}}$ is the observed Hessian evaluated at the posterior mode. Note that $\frac{\partial^2 \;\text{ln} \; p(\beta|\text{y})}{\partial \beta \partial \beta^T}$ is an 8x8 matrix with second derivatives on the diagonal and cross-derivatives $\frac{\partial^2 \; \text{ln} \; p(\beta|\text{y})}{\partial \beta_i \partial \beta_j}$ on the off-diagonal. It is actually not hard to compute this derivative by hand, but don't worry, we will let the computer do it numerically for you. Now, both $\tilde{\beta}$ and $J(\tilde{\beta})$ are computed by the `optim` function in R. See my code
[https://github.com/mattiasvillani/BayesLearnCourse/blob/master/Code/MainOptimizeSpam.zip](https://github.com/mattiasvillani/BayesLearnCourse/blob/master/Code/MainOptimizeSpam.zip)
where I have coded everything up for the spam prediction example (it also does probit regression, but that is not needed here). I want you to implement your own version of this. You can use my code as a template, but I want you to write your own file so that you understand every line of your code. Don't just copy my code. Use the prior $\beta \sim \mathcal{N}(0, \tau^2I)$, with $\tau = 10$. Your report should include your code as well as numerical values for $\tilde{\beta}$ and $J^{(-1)}_y\tilde{\beta}$ for the `WomanWork` data. Compute an approximate 95% credible interval for the variable NSmallChild. Would you say that this feature is an important determinant of the probability that a women works? \newline
$[$Hint: To verify that your results are reasonable, you can compare to you get by estimating the parameters using maximum likelihood: `glmModel <- glm(Work ~ 0 + ., data = WomenWork, family = binomial)`.$]$

\everymath{\color{mycolor}}
\newpage
\begin{equation*}
  \begin{split}
    P(y|X,\beta) =& \prod_{i=1}^{n} \frac{\left(\text{exp}(x_i^T \beta)\right)^{y_i}}{1 + \text{exp}(x_i^T \beta)} \\
    LogP(y|X,\beta) =& Log\prod_{i=1}^{n} \frac{\left(\text{exp}(x_i^T \beta)\right)^{y_i}}{1 + \text{exp}(x_i^T \beta)} \\
    =& \sum_{i=1}^{n}log\frac{\left(\text{exp}(x_i^T \beta)\right)^{y_i}}{1 + \text{exp}(x_i^T \beta)} \\
    =& \sum_{i = 1}^{n}x_i^T\beta y_i - \text{log}(1 + \text{exp}(x_i \beta))
  \end{split}
\end{equation*}
```{r 2_a}
```
- \textcolor{mycolor}{Yes, this feature is the main one that correlate with the women not working, since the posterior mode for the absolute value of the beta is relatively much higher than the other features and the negative sign is inverse relatioship with Work} 

(b) Write a function that simulates from the predictive distribution of the response variable in a logistic regression. Use your normal approximation from 2(a). Use that function to simulate and plot the predictive distribution for the `Work` variable for a 40 year old woman, with two children (3 and 9 years old), 8 years of education, 10 years of experience. and a husband with an income of 10. [Hints: The R package `mvtnorm` will again be handy. Remember my discussion on how Bayesian prediction can be done by simulation.]
```{r 2_b}
```


(c) Now, consider 10 women which all have the same features as the woman in 2(b). Rewrite your function and plot the predictive distribution for the number of women, out of these 10, that are working. [Hint: Which distribution can be
described as a sum of Bernoulli random variables?]
```{r 2_c}
```


